# LangChain å®è·µæŒ‡å—

## ğŸ”§ å¿«é€Ÿå¼€å§‹æ¨¡æ¿

### é¡¹ç›®åˆå§‹åŒ–

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir my_langchain_project
cd my_langchain_project

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install langchain langchain-openai langchain-community python-dotenv
pip install ipython jupyter  # å¯é€‰ï¼Œç”¨äºäº¤äº’å¼å¼€å‘

# é…ç½®ç¯å¢ƒå˜é‡
cat > .env << EOF
OPENAI_API_KEY=your-api-key-here
EOF
```

### é¡¹ç›®ç»“æ„
```
my_project/
â”œâ”€â”€ .env                    # ç¯å¢ƒå˜é‡
â”œâ”€â”€ .gitignore             # Git å¿½ç•¥
â”œâ”€â”€ requirements.txt       # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ README.md             # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chains.py         # é“¾å®šä¹‰
â”‚   â”œâ”€â”€ prompts.py        # æç¤ºæ¨¡æ¿
â”‚   â”œâ”€â”€ tools.py          # å·¥å…·å®šä¹‰
â”‚   â”œâ”€â”€ agents.py         # ä»£ç†å®ç°
â”‚   â””â”€â”€ utils.py          # å·¥å…·å‡½æ•°
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_chains.py
â”‚   â””â”€â”€ test_agents.py
â””â”€â”€ examples/
    â”œâ”€â”€ basic_rag.py
    â”œâ”€â”€ agent_example.py
    â””â”€â”€ langgraph_example.py
```

---

## ğŸ“ å¸¸ç”¨ä»£ç æ¨¡æ¿

### 1. åŸºç¡€ LLM è°ƒç”¨

```python
# src/chains.py
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import os
from dotenv import load_dotenv

load_dotenv()

def create_basic_chain():
    """åˆ›å»ºæœ€åŸºç¡€çš„ LLM é“¾"""
    model = ChatOpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        model="gpt-4",
        temperature=0.7
    )

    prompt = ChatPromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ã€‚\né—®é¢˜: {question}"
    )

    output_parser = StrOutputParser()

    # ä½¿ç”¨ LCEL ç»„åˆ
    chain = prompt | model | output_parser

    return chain

# ä½¿ç”¨
if __name__ == "__main__":
    chain = create_basic_chain()
    result = chain.invoke({"question": "ä»€ä¹ˆæ˜¯ Python?"})
    print(result)
```

### 2. å¤šè½®å¯¹è¯

```python
# src/chains.py
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate

def create_conversation_chain():
    """åˆ›å»ºæ”¯æŒå¤šè½®å¯¹è¯çš„é“¾"""
    model = ChatOpenAI()

    # æ„å»ºå¯¹è¯æ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ç¼–ç¨‹åŠ©æ‰‹"),
        ("user", "{input}")
    ])

    return prompt | model

def chat_with_history():
    """æ¼”ç¤ºå¤šè½®å¯¹è¯"""
    chain = create_conversation_chain()
    messages = []

    while True:
        user_input = input("User: ")
        if user_input.lower() == "exit":
            break

        # æ·»åŠ æ¶ˆæ¯
        result = chain.invoke({"input": user_input})
        print(f"Assistant: {result.content}")

# ä½¿ç”¨
if __name__ == "__main__":
    chat_with_history()
```

### 3. å·¥å…·å®šä¹‰ä¸ä½¿ç”¨

```python
# src/tools.py
from langchain_core.tools import tool
from typing import Any
import requests
import json

@tool
def search_web(query: str) -> str:
    """
    æœç´¢ç½‘ç»œä¿¡æ¯

    Args:
        query: æœç´¢æŸ¥è¯¢

    Returns:
        æœç´¢ç»“æœ
    """
    # è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå®ç°
    return f"å…³äº '{query}' çš„æœç´¢ç»“æœ..."

@tool
def calculate(expression: str) -> float:
    """
    è®¡ç®—æ•°å­¦è¡¨è¾¾å¼

    Args:
        expression: æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚ "2+3*4"

    Returns:
        è®¡ç®—ç»“æœ
    """
    try:
        result = eval(expression)
        return float(result)
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {e}"

@tool
def get_current_weather(location: str) -> dict:
    """
    è·å–å½“å‰å¤©æ°”ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰

    Args:
        location: åœ°ç‚¹åç§°

    Returns:
        å¤©æ°”ä¿¡æ¯
    """
    weather_data = {
        "åŒ—äº¬": {"æ¸©åº¦": 20, "å¤©æ°”": "æ™´æœ—"},
        "ä¸Šæµ·": {"æ¸©åº¦": 18, "å¤©æ°”": "å¤šäº‘"},
        "æ·±åœ³": {"æ¸©åº¦": 25, "å¤©æ°”": "æ™´æœ—"}
    }
    return weather_data.get(location, {"æ¸©åº¦": "æœªçŸ¥", "å¤©æ°”": "æ— æ•°æ®"})

def get_tools():
    """è·å–æ‰€æœ‰å·¥å…·åˆ—è¡¨"""
    return [search_web, calculate, get_current_weather]

# æµ‹è¯•å·¥å…·
if __name__ == "__main__":
    print("æœç´¢:", search_web.invoke("Python"))
    print("è®¡ç®—:", calculate.invoke("2+3*4"))
    print("å¤©æ°”:", get_current_weather.invoke("åŒ—äº¬"))
```

### 4. RAG ç³»ç»Ÿ

```python
# src/rag.py
from langchain_community.document_loaders import DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
import os

class RAGSystem:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
        self.llm = ChatOpenAI(temperature=0)
        self.vectorstore = None
        self.qa_chain = None

    def load_documents(self, doc_dir: str):
        """åŠ è½½æ–‡æ¡£"""
        loader = DirectoryLoader(doc_dir, glob="*.md")
        documents = loader.load()
        return documents

    def process_documents(self, documents):
        """å¤„ç†æ–‡æ¡£"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ï¼Œ", ""]
        )
        chunks = splitter.split_documents(documents)
        return chunks

    def build_vectorstore(self, doc_dir: str):
        """æ„å»ºå‘é‡å­˜å‚¨"""
        documents = self.load_documents(doc_dir)
        chunks = self.process_documents(documents)

        self.vectorstore = FAISS.from_documents(
            chunks,
            self.embeddings
        )

    def setup_qa_chain(self):
        """è®¾ç½® QA é“¾"""
        retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": 3}
        )

        prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""ä½¿ç”¨ä»¥ä¸‹æ–‡ä»¶å›ç­”é—®é¢˜ã€‚

æ–‡ä»¶å†…å®¹:
{context}

é—®é¢˜: {question}

ç­”æ¡ˆ:"""
        )

        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": prompt_template}
        )

    def query(self, question: str) -> str:
        """æŸ¥è¯¢"""
        if not self.qa_chain:
            raise ValueError("Please setup QA chain first")
        return self.qa_chain.run(question)

# ä½¿ç”¨
if __name__ == "__main__":
    rag = RAGSystem()
    rag.build_vectorstore("./documents")
    rag.setup_qa_chain()

    answer = rag.query("æ–‡æ¡£ä¸­è®²äº†ä»€ä¹ˆ?")
    print(answer)
```

### 5. ä»£ç†å®ç°

```python
# src/agents.py
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langchain_core.messages import BaseMessage, HumanMessage
from typing import TypedDict, Annotated
import operator

# å®šä¹‰å·¥å…·
@tool
def search(query: str) -> str:
    """æœç´¢ä¿¡æ¯"""
    return f"æœç´¢ '{query}' çš„ç»“æœ..."

@tool
def calculator(expression: str) -> str:
    """è®¡ç®—è¡¨è¾¾å¼"""
    return str(eval(expression))

class AgentState(TypedDict):
    messages: Annotated[list[BaseMessage], operator.add]

def create_agent():
    """åˆ›å»ºå·¥å…·è°ƒç”¨ä»£ç†"""
    model = ChatOpenAI(model="gpt-4")
    tools = [search, calculator]

    # ç»‘å®šå·¥å…·
    model_with_tools = model.bind_tools(tools)

    # å®šä¹‰èŠ‚ç‚¹å‡½æ•°
    def agent(state: AgentState):
        """ä»£ç†èŠ‚ç‚¹"""
        messages = state["messages"]
        response = model_with_tools.invoke(messages)
        return {"messages": [response]}

    def should_continue(state: AgentState):
        """å†³å®šæ˜¯å¦ç»§ç»­"""
        messages = state["messages"]
        last_message = messages[-1]

        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œç»§ç»­
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            return "tools"
        return END

    # æ„å»ºå›¾
    workflow = StateGraph(AgentState)
    workflow.add_node("agent", agent)
    workflow.add_node("tools", ToolNode(tools))

    workflow.add_edge(START, "agent")
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {"tools": "tools", END: END}
    )
    workflow.add_edge("tools", "agent")

    return workflow.compile()

# ä½¿ç”¨
if __name__ == "__main__":
    graph = create_agent()

    result = graph.invoke({
        "messages": [HumanMessage(content="è®¡ç®— 2+3*4")]
    })

    print(result["messages"][-1].content)
```

### 6. LangGraph å·¥ä½œæµ

```python
# src/langgraph_example.py
from langgraph.graph import StateGraph, START, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from typing import TypedDict, Annotated
import operator

class State(TypedDict):
    messages: Annotated[list[BaseMessage], operator.add]

def create_simple_graph():
    """åˆ›å»ºç®€å•çš„ LangGraph å·¥ä½œæµ"""
    model = ChatOpenAI()

    def node_1(state: State):
        """ç¬¬ä¸€ä¸ªèŠ‚ç‚¹"""
        messages = state["messages"]
        response = model.invoke(messages)
        return {"messages": [response]}

    def node_2(state: State):
        """ç¬¬äºŒä¸ªèŠ‚ç‚¹"""
        messages = state["messages"]
        # è¿›è¡ŒæŸäº›å¤„ç†
        return state

    def route(state: State) -> str:
        """è·¯ç”±å‡½æ•°"""
        messages = state["messages"]
        last_message = messages[-1]

        if "é—®é¢˜" in last_message.content:
            return "node_1"
        return "node_2"

    # æ„å»ºå›¾
    graph = StateGraph(State)
    graph.add_node("node_1", node_1)
    graph.add_node("node_2", node_2)

    graph.add_edge(START, "node_1")
    graph.add_conditional_edges(
        "node_1",
        route,
        {"node_1": "node_1", "node_2": "node_2"}
    )
    graph.add_edge("node_2", END)

    return graph.compile()

# ä½¿ç”¨
if __name__ == "__main__":
    graph = create_simple_graph()
    result = graph.invoke({
        "messages": [HumanMessage(content="è¿™æ˜¯ä¸€ä¸ªé—®é¢˜")]
    })
    print(result)
```

---

## ğŸ§ª æµ‹è¯•ä¸è¯„ä¼°

### å•å…ƒæµ‹è¯•æ¨¡æ¿

```python
# tests/test_chains.py
import unittest
from src.chains import create_basic_chain

class TestChains(unittest.TestCase):
    def setUp(self):
        self.chain = create_basic_chain()

    def test_basic_chain_returns_string(self):
        """æµ‹è¯•åŸºç¡€é“¾è¿”å›å­—ç¬¦ä¸²"""
        result = self.chain.invoke({"question": "ä»€ä¹ˆæ˜¯ Python?"})
        self.assertIsInstance(result, str)
        self.assertGreater(len(result), 0)

    def test_chain_handles_edge_cases(self):
        """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
        result = self.chain.invoke({"question": ""})
        self.assertIsInstance(result, str)

if __name__ == "__main__":
    unittest.main()
```

### è¯„ä¼°å·¥å…·

```python
# src/evaluation.py
from typing import List, Dict
import json

class RAGEvaluator:
    """RAG ç³»ç»Ÿè¯„ä¼°å·¥å…·"""

    @staticmethod
    def evaluate_retrieval(retrieved_docs: List[str],
                          expected_docs: List[str]) -> Dict:
        """è¯„ä¼°æ£€ç´¢è´¨é‡"""
        if not retrieved_docs:
            return {"precision": 0, "recall": 0}

        retrieved_set = set(retrieved_docs)
        expected_set = set(expected_docs)

        intersection = retrieved_set & expected_set

        precision = len(intersection) / len(retrieved_set) if retrieved_set else 0
        recall = len(intersection) / len(expected_set) if expected_set else 0

        return {
            "precision": precision,
            "recall": recall,
            "f1": 2 * (precision * recall) / (precision + recall)
                  if (precision + recall) > 0 else 0
        }

    @staticmethod
    def evaluate_generation(generated: str,
                           reference: str) -> Dict:
        """è¯„ä¼°ç”Ÿæˆè´¨é‡"""
        # ç®€å•çš„ç›¸ä¼¼æ€§è¯„ä¼°
        gen_words = set(generated.lower().split())
        ref_words = set(reference.lower().split())

        overlap = gen_words & ref_words
        similarity = len(overlap) / len(gen_words) if gen_words else 0

        return {
            "similarity": similarity,
            "length_ratio": len(generated) / len(reference)
                           if reference else 0
        }

# ä½¿ç”¨
if __name__ == "__main__":
    evaluator = RAGEvaluator()

    metrics = evaluator.evaluate_retrieval(
        ["doc1", "doc2"],
        ["doc1", "doc3"]
    )
    print(metrics)
```

---

## ğŸ› å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### é—®é¢˜ 1: API å¯†é’¥é”™è¯¯

```python
# è§£å†³æ–¹æ¡ˆ 1: ä½¿ç”¨ç¯å¢ƒå˜é‡
import os
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    raise ValueError("OPENAI_API_KEY not found in environment")

# è§£å†³æ–¹æ¡ˆ 2: ç›´æ¥ä¼ é€’
model = ChatOpenAI(api_key="your-key")
```

### é—®é¢˜ 2: Token é™åˆ¶

```python
# è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–åˆ†å‰²æ–‡æ¡£
from langchain_openai import ChatOpenAI

# é€‰æ‹©è¾ƒå°çš„æ¨¡å‹
model = ChatOpenAI(model="gpt-3.5-turbo")

# æˆ–é™åˆ¶è¾“å‡ºé•¿åº¦
model = ChatOpenAI(max_tokens=500)
```

### é—®é¢˜ 3: å‘é‡å­˜å‚¨æ€§èƒ½

```python
# è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æœ‰æ•ˆçš„æœç´¢å‚æ•°
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 3}  # åªè¿”å›å‰ 3 ä¸ªç»“æœ
)

# æˆ–ä½¿ç”¨ MMR æœç´¢
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 3, "fetch_k": 10}
)
```

### é—®é¢˜ 4: å¤„ç†è¶…é•¿è¾“å…¥

```python
# è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æ–‡æœ¬åˆ†å‰²
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

chunks = splitter.split_text(long_text)
```

---

## ğŸ“Š è°ƒè¯•æŠ€å·§

### å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import logging

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger("langchain")
logger.setLevel(logging.DEBUG)

# ç°åœ¨æ‰€æœ‰ LangChain æ“ä½œéƒ½ä¼šè¾“å‡ºè¯¦ç»†æ—¥å¿—
model = ChatOpenAI()
result = model.invoke("test")
```

### è¿½è¸ªæ‰§è¡Œæ­¥éª¤

```python
from langchain.callbacks import StdOutCallbackHandler

# æ–¹æ³• 1: å…¨å±€å›è°ƒ
callbacks = [StdOutCallbackHandler()]

# æ–¹æ³• 2: é“¾çº§å›è°ƒ
chain.invoke(input, config={"callbacks": callbacks})
```

### ä½¿ç”¨ LangSmith

```python
import os

# å¯ç”¨ LangSmith è¿½è¸ª
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-langsmith-key"

# ç°åœ¨æ‰€æœ‰æ“ä½œéƒ½ä¼šè¢«è¿½è¸ªåˆ° LangSmith dashboard
```

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### ç¼“å­˜ä¼˜åŒ–

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

# å¯ç”¨ç¼“å­˜
set_llm_cache(InMemoryCache())

# é‡å¤è°ƒç”¨ä¼šä½¿ç”¨ç¼“å­˜ç»“æœ
model = ChatOpenAI()
result1 = model.invoke("same input")
result2 = model.invoke("same input")  # ä½¿ç”¨ç¼“å­˜
```

### æ‰¹å¤„ç†ä¼˜åŒ–

```python
# ä½¿ç”¨ batch æ–¹æ³•è€Œä¸æ˜¯å¾ªç¯
inputs = ["input1", "input2", "input3"]

# ä½æ•ˆï¼šå¾ªç¯
results = []
for inp in inputs:
    results.append(model.invoke(inp))

# é«˜æ•ˆï¼šæ‰¹å¤„ç†
results = model.batch(inputs)
```

### å¼‚æ­¥ä¼˜åŒ–

```python
import asyncio

async def process_multiple():
    # å¼‚æ­¥å¹¶å‘å¤„ç†
    tasks = [
        model.ainvoke("input1"),
        model.ainvoke("input2"),
        model.ainvoke("input3")
    ]
    results = await asyncio.gather(*tasks)
    return results

# è¿è¡Œ
results = asyncio.run(process_multiple())
```

---

## ğŸ”‘ æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†

```python
try:
    result = chain.invoke(input)
except Exception as e:
    print(f"Error: {e}")
    # æä¾›é™çº§æ–¹æ¡ˆ
    result = get_default_response()
```

### 2. æç¤ºä¼˜åŒ–

```python
# ä¸å¥½ï¼šæ¨¡ç³Šçš„æç¤º
"ç¿»è¯‘è¿™ä¸ªæ–‡æœ¬"

# å¥½ï¼šæ¸…æ™°çš„æç¤º
"""å°†ä»¥ä¸‹è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ã€‚
ç¡®ä¿ï¼š
1. å‡†ç¡®è¡¨è¾¾åŸæ„
2. è‡ªç„¶æµç•…
3. ä¿ç•™æŠ€æœ¯æœ¯è¯­

æ–‡æœ¬ï¼š{text}"""
```

### 3. æ¨¡å‹é€‰æ‹©

```python
# æˆæœ¬ä¼˜åŒ–ï¼šé€‰æ‹©åˆé€‚çš„æ¨¡å‹
model = ChatOpenAI(model="gpt-3.5-turbo")  # ä¾¿å®œ

# è´¨é‡ä¼˜åŒ–ï¼šé€‰æ‹©æ›´å¼ºçš„æ¨¡å‹
model = ChatOpenAI(model="gpt-4")  # è´µä½†æ›´å¥½

# å¹³è¡¡ï¼š
model = ChatOpenAI(model="gpt-4-turbo")  # æŠ˜ä¸­æ–¹æ¡ˆ
```

### 4. å‚æ•°è°ƒæ•´

```python
# åˆ›æ„ä»»åŠ¡ï¼šé«˜æ¸©åº¦
model = ChatOpenAI(temperature=0.8)

# åˆ†æä»»åŠ¡ï¼šä½æ¸©åº¦
model = ChatOpenAI(temperature=0.1)

# å¹³è¡¡ï¼š
model = ChatOpenAI(temperature=0.5)
```

---

## ğŸ“š æ¨èé¡¹ç›®æ¡ˆä¾‹

### é¡¹ç›® 1: ä¼ä¸šçŸ¥è¯†åº“ QA

```python
# å®Œæ•´å®ç°éª¨æ¶
class EnterpriseKnowledgeBase:
    def __init__(self):
        self.rag = RAGSystem()

    def load_knowledge(self, path):
        self.rag.build_vectorstore(path)

    def answer_question(self, question):
        return self.rag.query(question)
```

### é¡¹ç›® 2: AI ä»£ç å®¡æŸ¥å·¥å…·

```python
class CodeReviewer:
    def __init__(self):
        self.model = ChatOpenAI()
        self.tools = [analyze_complexity, check_bugs]

    def review_code(self, code):
        # ä½¿ç”¨ä»£ç†åˆ†æä»£ç 
        pass
```

### é¡¹ç›® 3: å¤šè¯­è¨€æ–‡æ¡£ç¿»è¯‘

```python
class MultilingualTranslator:
    def __init__(self, target_languages):
        self.languages = target_languages

    def translate_document(self, doc_path):
        # å¹¶è¡Œç¿»è¯‘åˆ°å¤šä¸ªè¯­è¨€
        pass
```

---

å¼€å§‹å®è·µå§ï¼ğŸš€
